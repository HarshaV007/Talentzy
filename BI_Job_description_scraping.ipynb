{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas_profiling as Report\n",
    "import time \n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://www.indeed.co.in/jobs?q=business+intelligence&l=')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soup = BeautifulSoup(page.content,'html.parser')\n",
    "print(soup.title)\n",
    "print(soup.prettify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def extract_job_title_from_result(soup):\n",
    "#    jobs = []\n",
    "#    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
    "#        for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
    "#            jobs.append(a[\"title\"])\n",
    "#    return(jobs)\n",
    "#extract_job_title_from_result(soup)\n",
    "#jobs = []\n",
    "#companies = []\n",
    "#summaries = []\n",
    "#for i in ('10','20','30','40','50','60','80','90'):\n",
    "#    \n",
    "#    \n",
    "#    page = requests.get('https://www.indeed.co.in/jobs?q=business+intelligence&l=Hyderabad%2C+Telangana&start='+i+'')\n",
    "#    soup = BeautifulSoup(page.content,'html.parser')\n",
    "#    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
    "#        for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
    "#            jobs.append(a[\"title\"])\n",
    "#    \n",
    "#    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
    "#        company = div.find_all(name=\"span\", attrs={\"class\":\"company\"})\n",
    "#        if len(company) > 0:\n",
    "#            for b in company:\n",
    "#                companies.append(b.text.strip())\n",
    "#        else:\n",
    "#            sec_try = div.find_all(name=\"span\", attrs={'class':\"result-link-source\"})\n",
    "#            for span in sec_try:\n",
    "#                companies.append(span.text.strip())\n",
    "#    spans = soup.findAll('div', attrs={'class': 'summary'})\n",
    "#    for span in spans:\n",
    "#        summaries.append(span.text.strip())\n",
    "#d={'Job_Title':jobs,'Company':companies,'Job_description':summaries}\n",
    "#data=pd.DataFrame(data=d)       \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(\"./chromedriver\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=[\"Title\",\"Location\",\"Company\",\"Salary\",\"Sponsored\",\"Description\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,500,10):\n",
    "    driver.get('https://www.indeed.co.in/jobs?q=business+intelligence&start='+str(i))\n",
    "    jobs = []\n",
    "    driver.implicitly_wait(10)\n",
    "    time.sleep(10)\n",
    "\n",
    "\n",
    "    for job in driver.find_elements_by_class_name('result'):\n",
    "\n",
    "        soup = BeautifulSoup(job.get_attribute('innerHTML'),'html.parser')\n",
    "\n",
    "        try:\n",
    "            title = soup.find(\"a\",class_=\"jobtitle\").text.replace(\"\\n\",\"\").strip()\n",
    "\n",
    "        except:\n",
    "            title = 'None'\n",
    "\n",
    "        try:\n",
    "            location = soup.find(class_=\"location\").text\n",
    "        except:\n",
    "            location = 'None'\n",
    "\n",
    "        try:\n",
    "            company = soup.find(class_=\"company\").text.replace(\"\\n\",\"\").strip()\n",
    "        except:\n",
    "            company = 'None'\n",
    "\n",
    "        try:\n",
    "            salary = soup.find(class_=\"salary\").text.replace(\"\\n\",\"\").strip()\n",
    "        except:\n",
    "            salary = 'None'\n",
    "\n",
    "        try:\n",
    "            sponsored = soup.find(class_=\"sponsoredGray\").text\n",
    "            sponsored = \"Sponsored\"\n",
    "        except:\n",
    "            sponsored = \"Organic\"\n",
    "\n",
    "        sum_div = job.find_elements_by_class_name(\"summary\")[0]\n",
    "        try:\n",
    "            sum_div.click()\n",
    "        except:\n",
    "            close_button = driver.find_elements_by_class_name('popover-x-button-close')[0]\n",
    "            close_button.click()\n",
    "            sum_div.click()\n",
    "        job_desc = driver.find_element_by_id('vjs-desc').text\n",
    "\n",
    "        df = df.append({'Title':title,'Location':location,\"Company\":company,\"Salary\":salary,\n",
    "                        \"Sponsored\":sponsored,\"Description\":job_desc},ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Title']=='Data Warehouse-Senior/Junior']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Business_Intelligence.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
